{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dc0dfa14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc0dfa14",
        "outputId": "27fa025b-39c8-4b99-8490-dc20954364d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "82637b61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82637b61",
        "outputId": "f67470dc-8eac-434b-c8bd-b0bd1db55795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Power Grid Optimization\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Power\\ Grid\\ Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6699be23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6699be23",
        "outputId": "028af53e-e526-4666-9903-aa0b70b441aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting pandapower\n",
            "  Downloading pandapower-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from pandapower) (2.2.2)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from pandapower) (3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pandapower) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pandapower) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pandapower) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pandapower) (4.67.1)\n",
            "Collecting deepdiff (from pandapower)\n",
            "  Downloading deepdiff-8.5.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting geojson (from pandapower)\n",
            "  Downloading geojson-3.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pandapower) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from pandapower) (4.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->pandapower) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->pandapower) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->pandapower) (2025.2)\n",
            "Collecting orderly-set<6,>=5.4.1 (from deepdiff->pandapower)\n",
            "  Downloading orderly_set-5.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->pandapower) (1.17.0)\n",
            "Downloading pandapower-3.1.1-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepdiff-8.5.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading geojson-3.2.0-py3-none-any.whl (15 kB)\n",
            "Downloading orderly_set-5.4.1-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: orderly-set, geojson, deepdiff, pandapower\n",
            "Successfully installed deepdiff-8.5.0 geojson-3.2.0 orderly-set-5.4.1 pandapower-3.1.1\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torch-geometric\n",
        "!pip install pandas openpyxl\n",
        "!pip install pandapower\n",
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "905f033d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "905f033d",
        "outputId": "2465905c-63a2-404a-bbee-d53b0d146c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/drive/MyDrive/Power%20Grid%20Optimization\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: powergrid_torch\n",
            "  Building editable for powergrid_torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for powergrid_torch: filename=powergrid_torch-1.0.0-0.editable-py3-none-any.whl size=1466 sha256=dbe390c52a0e8599b2d3e45ede8b3aed2894fe23023f70f1f80a997c1b5ca8b0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8w4a_s8a/wheels/59/7e/dc/e7c694b3435d46bfb0bc9c3b277d4f33fd0c87b4e5e0a16cb4\n",
            "Successfully built powergrid_torch\n",
            "Installing collected packages: powergrid_torch\n",
            "Successfully installed powergrid_torch-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import powergrid_torch"
      ],
      "metadata": {
        "id": "YoOrH8LFSltz"
      },
      "id": "YoOrH8LFSltz",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ed4bae3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed4bae3c",
        "outputId": "6a8ca16f-99f5-44d8-d5c9-3ac58113c4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Power Grid Optimization/powergrid_torch/data/14Bus\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Power\\ Grid\\ Optimization/powergrid_torch/data/14Bus/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5364509a",
      "metadata": {
        "id": "5364509a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from powergrid_torch.utils import pre_processing\n",
        "import os\n",
        "from powergrid_torch.models.gnn import GNNModel\n",
        "from powergrid_torch.models.transformer import GraphTransformerModel\n",
        "from powergrid_torch.training import train\n",
        "from powergrid_torch.utils.extras import summary\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "744edd6e",
      "metadata": {
        "id": "744edd6e"
      },
      "outputs": [],
      "source": [
        "path_data = os.getcwd() # os.path.join(os.getcwd(), r'../data/14Bus/')\n",
        "train_df = pd.read_excel(os.path.join(path_data, 'PF_Dataset_1.xlsx')).values\n",
        "val_df = pd.read_excel(os.path.join(path_data, 'PF_Dataset_2.xlsx')).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b9d557fc",
      "metadata": {
        "id": "b9d557fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20aff8a-0d6c-4523-f85d-ee48e42bb9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------Data preparation completed successfully--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "stats_result = pre_processing.stats(train_df, val_df, 1., 1., 14)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "for idx, val in enumerate(stats_result):\n",
        "    val = val.to(device=device)\n",
        "    stats_result[idx] = val\n",
        "\n",
        "y_norm_train = stats_result[1]\n",
        "y_train_mean = stats_result[3]\n",
        "y_train_std = stats_result[5]\n",
        "\n",
        "y_norm_val = stats_result[7]\n",
        "y_val_mean = stats_result[9]\n",
        "y_val_std = stats_result[11]\n",
        "\n",
        "train_loader, val_loader = pre_processing.main(train_df, val_df, 1., 1., 14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "648c4400",
      "metadata": {
        "id": "648c4400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78e4a17-d1ae-450e-8582-4502c9e52a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = pre_processing.get_default_device()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0644462a",
      "metadata": {
        "id": "0644462a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def model_run(gnn_type = 'GCN', n_epochs = 5, lambda_l2 = 1e-6, n_bus = 14):\n",
        "\n",
        "    if (gnn_type != 'GraphTransformer'):\n",
        "        model = pre_processing.to_device(GNNModel(in_channels=5, hidden_channels=8, out_channels=2, n_bus=14, gnn_type=gnn_type, is_batch_norm='False', is_dropout=0), device)\n",
        "\n",
        "    elif (gnn_type == 'GraphTransformer'):\n",
        "        model = pre_processing.to_device(GraphTransformerModel(in_channels=5, hidden_channels=8, out_channels=2, n_transformer_layers=2, n_heads=3, concat_heads=True), device)\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=lambda_l2)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)\n",
        "\n",
        "    summary(model)\n",
        "    history = train.fit(n_epochs, 0.001, model, train_loader, val_loader, optimizer, scheduler, y_train_mean, y_train_std, y_val_mean, y_val_std, n_bus)\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = pre_processing.DeviceDataLoader(train_loader, device)\n",
        "val_loader = pre_processing.DeviceDataLoader(val_loader, device)"
      ],
      "metadata": {
        "id": "5smw0W1IMEGR"
      },
      "id": "5smw0W1IMEGR",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "history_transformer = model_run(\"GraphTransformer\", n_epochs=400)\n",
        "end = time.time()\n",
        "print(f\"Time Taken = {round(end-start, 2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57kRtZ_NMGuz",
        "outputId": "f70385bb-fb15-4c02-ba49-d6d474e32dc6"
      },
      "id": "57kRtZ_NMGuz",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model Summary ---\n",
            "GraphTransformerModel(\n",
            "  (transformer_layers): ModuleList(\n",
            "    (0): TransformerConv(5, 8, heads=3)\n",
            "    (1): TransformerConv(24, 8, heads=3)\n",
            "  )\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            "  (final_projection): Linear(in_features=24, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "--- Detailed Parameter Summary ---\n",
            "transformer_layers.0.lin_key.weight                [24, 5]              120\n",
            "transformer_layers.0.lin_key.bias                  [24]                 24\n",
            "transformer_layers.0.lin_query.weight              [24, 5]              120\n",
            "transformer_layers.0.lin_query.bias                [24]                 24\n",
            "transformer_layers.0.lin_value.weight              [24, 5]              120\n",
            "transformer_layers.0.lin_value.bias                [24]                 24\n",
            "transformer_layers.0.lin_skip.weight               [24, 5]              120\n",
            "transformer_layers.0.lin_skip.bias                 [24]                 24\n",
            "transformer_layers.1.lin_key.weight                [24, 24]             576\n",
            "transformer_layers.1.lin_key.bias                  [24]                 24\n",
            "transformer_layers.1.lin_query.weight              [24, 24]             576\n",
            "transformer_layers.1.lin_query.bias                [24]                 24\n",
            "transformer_layers.1.lin_value.weight              [24, 24]             576\n",
            "transformer_layers.1.lin_value.bias                [24]                 24\n",
            "transformer_layers.1.lin_skip.weight               [24, 24]             576\n",
            "transformer_layers.1.lin_skip.bias                 [24]                 24\n",
            "final_projection.weight                            [2, 24]              48\n",
            "final_projection.bias                              [2]                  2\n",
            "\n",
            " Total Trainable Params: 3026\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "Epoch[0], train_loss: 0.91043, val_loss: 0.9371\n",
            "Epoch[1], train_loss: 0.87023, val_loss: 0.89912\n",
            "Epoch[2], train_loss: 0.83694, val_loss: 0.86655\n",
            "Epoch[3], train_loss: 0.80774, val_loss: 0.83738\n",
            "Epoch[4], train_loss: 0.78115, val_loss: 0.81066\n",
            "Epoch[5], train_loss: 0.75665, val_loss: 0.7862\n",
            "Epoch[6], train_loss: 0.73444, val_loss: 0.76461\n",
            "Epoch[7], train_loss: 0.71519, val_loss: 0.74655\n",
            "Epoch[8], train_loss: 0.69955, val_loss: 0.73261\n",
            "Epoch[9], train_loss: 0.68773, val_loss: 0.72254\n",
            "Epoch[10], train_loss: 0.67925, val_loss: 0.71566\n",
            "Epoch[11], train_loss: 0.67364, val_loss: 0.71133\n",
            "Epoch[12], train_loss: 0.6701, val_loss: 0.70867\n",
            "Epoch[13], train_loss: 0.66785, val_loss: 0.707\n",
            "Epoch[14], train_loss: 0.66634, val_loss: 0.70585\n",
            "Epoch[15], train_loss: 0.66524, val_loss: 0.70497\n",
            "Epoch[16], train_loss: 0.66438, val_loss: 0.70422\n",
            "Epoch[17], train_loss: 0.66365, val_loss: 0.70355\n",
            "Epoch[18], train_loss: 0.66301, val_loss: 0.70293\n",
            "Epoch[19], train_loss: 0.66241, val_loss: 0.70234\n",
            "Epoch[20], train_loss: 0.66185, val_loss: 0.70176\n",
            "Epoch[21], train_loss: 0.66132, val_loss: 0.70121\n",
            "Epoch[22], train_loss: 0.66082, val_loss: 0.70067\n",
            "Epoch[23], train_loss: 0.66034, val_loss: 0.70015\n",
            "Epoch[24], train_loss: 0.65988, val_loss: 0.69964\n",
            "Epoch[25], train_loss: 0.65943, val_loss: 0.69914\n",
            "Epoch[26], train_loss: 0.659, val_loss: 0.69866\n",
            "Epoch[27], train_loss: 0.65858, val_loss: 0.69819\n",
            "Epoch[28], train_loss: 0.65818, val_loss: 0.69773\n",
            "Epoch[29], train_loss: 0.6578, val_loss: 0.69729\n",
            "Epoch[30], train_loss: 0.65742, val_loss: 0.69688\n",
            "Epoch[31], train_loss: 0.65707, val_loss: 0.69646\n",
            "Epoch[32], train_loss: 0.65671, val_loss: 0.69605\n",
            "Epoch[33], train_loss: 0.65636, val_loss: 0.69565\n",
            "Epoch[34], train_loss: 0.65603, val_loss: 0.69526\n",
            "Epoch[35], train_loss: 0.65569, val_loss: 0.69488\n",
            "Epoch[36], train_loss: 0.65537, val_loss: 0.69451\n",
            "Epoch[37], train_loss: 0.65505, val_loss: 0.69415\n",
            "Epoch[38], train_loss: 0.65474, val_loss: 0.6938\n",
            "Epoch[39], train_loss: 0.65443, val_loss: 0.69345\n",
            "Epoch[40], train_loss: 0.65413, val_loss: 0.69311\n",
            "Epoch[41], train_loss: 0.65382, val_loss: 0.69277\n",
            "Epoch[42], train_loss: 0.65353, val_loss: 0.69244\n",
            "Epoch[43], train_loss: 0.65324, val_loss: 0.69211\n",
            "Epoch[44], train_loss: 0.65295, val_loss: 0.69179\n",
            "Epoch[45], train_loss: 0.65266, val_loss: 0.69147\n",
            "Epoch[46], train_loss: 0.65238, val_loss: 0.69115\n",
            "Epoch[47], train_loss: 0.6521, val_loss: 0.69084\n",
            "Epoch[48], train_loss: 0.65183, val_loss: 0.69053\n",
            "Epoch[49], train_loss: 0.65155, val_loss: 0.69022\n",
            "Epoch[50], train_loss: 0.65127, val_loss: 0.68992\n",
            "Epoch[51], train_loss: 0.651, val_loss: 0.68963\n",
            "Epoch[52], train_loss: 0.65073, val_loss: 0.68934\n",
            "Epoch[53], train_loss: 0.65046, val_loss: 0.68904\n",
            "Epoch[54], train_loss: 0.65019, val_loss: 0.68875\n",
            "Epoch[55], train_loss: 0.64992, val_loss: 0.68846\n",
            "Epoch[56], train_loss: 0.64965, val_loss: 0.68817\n",
            "Epoch[57], train_loss: 0.64938, val_loss: 0.68789\n",
            "Epoch[58], train_loss: 0.64911, val_loss: 0.6876\n",
            "Epoch[59], train_loss: 0.64884, val_loss: 0.68732\n",
            "Epoch[60], train_loss: 0.64858, val_loss: 0.68703\n",
            "Epoch[61], train_loss: 0.64832, val_loss: 0.68676\n",
            "Epoch[62], train_loss: 0.64806, val_loss: 0.68648\n",
            "Epoch[63], train_loss: 0.64781, val_loss: 0.68621\n",
            "Epoch[64], train_loss: 0.64755, val_loss: 0.68594\n",
            "Epoch[65], train_loss: 0.6473, val_loss: 0.68568\n",
            "Epoch[66], train_loss: 0.64705, val_loss: 0.68541\n",
            "Epoch[67], train_loss: 0.6468, val_loss: 0.68514\n",
            "Epoch[68], train_loss: 0.64655, val_loss: 0.68487\n",
            "Epoch[69], train_loss: 0.6463, val_loss: 0.68459\n",
            "Epoch[70], train_loss: 0.64605, val_loss: 0.68432\n",
            "Epoch[71], train_loss: 0.6458, val_loss: 0.68405\n",
            "Epoch[72], train_loss: 0.64554, val_loss: 0.68379\n",
            "Epoch[73], train_loss: 0.64529, val_loss: 0.68352\n",
            "Epoch[74], train_loss: 0.64504, val_loss: 0.68325\n",
            "Epoch[75], train_loss: 0.64479, val_loss: 0.68299\n",
            "Epoch[76], train_loss: 0.64454, val_loss: 0.68272\n",
            "Epoch[77], train_loss: 0.64428, val_loss: 0.68245\n",
            "Epoch[78], train_loss: 0.64402, val_loss: 0.68218\n",
            "Epoch[79], train_loss: 0.64376, val_loss: 0.68191\n",
            "Epoch[80], train_loss: 0.6435, val_loss: 0.68164\n",
            "Epoch[81], train_loss: 0.64324, val_loss: 0.68136\n",
            "Epoch[82], train_loss: 0.64298, val_loss: 0.68109\n",
            "Epoch[83], train_loss: 0.64271, val_loss: 0.68081\n",
            "Epoch[84], train_loss: 0.64245, val_loss: 0.68054\n",
            "Epoch[85], train_loss: 0.64218, val_loss: 0.68026\n",
            "Epoch[86], train_loss: 0.64191, val_loss: 0.67999\n",
            "Epoch[87], train_loss: 0.64165, val_loss: 0.67971\n",
            "Epoch[88], train_loss: 0.64138, val_loss: 0.67944\n",
            "Epoch[89], train_loss: 0.64111, val_loss: 0.67915\n",
            "Epoch[90], train_loss: 0.64084, val_loss: 0.67887\n",
            "Epoch[91], train_loss: 0.64056, val_loss: 0.67858\n",
            "Epoch[92], train_loss: 0.64029, val_loss: 0.6783\n",
            "Epoch[93], train_loss: 0.64001, val_loss: 0.67802\n",
            "Epoch[94], train_loss: 0.63974, val_loss: 0.67773\n",
            "Epoch[95], train_loss: 0.63947, val_loss: 0.67745\n",
            "Epoch[96], train_loss: 0.63919, val_loss: 0.67717\n",
            "Epoch[97], train_loss: 0.63891, val_loss: 0.67687\n",
            "Epoch[98], train_loss: 0.63863, val_loss: 0.67658\n",
            "Epoch[99], train_loss: 0.63835, val_loss: 0.67629\n",
            "Epoch[100], train_loss: 0.63807, val_loss: 0.67599\n",
            "Epoch[101], train_loss: 0.63778, val_loss: 0.6757\n",
            "Epoch[102], train_loss: 0.63749, val_loss: 0.6754\n",
            "Epoch[103], train_loss: 0.6372, val_loss: 0.67511\n",
            "Epoch[104], train_loss: 0.63691, val_loss: 0.67481\n",
            "Epoch[105], train_loss: 0.63662, val_loss: 0.67452\n",
            "Epoch[106], train_loss: 0.63633, val_loss: 0.67421\n",
            "Epoch[107], train_loss: 0.63603, val_loss: 0.67391\n",
            "Epoch[108], train_loss: 0.63573, val_loss: 0.6736\n",
            "Epoch[109], train_loss: 0.63543, val_loss: 0.6733\n",
            "Epoch[110], train_loss: 0.63513, val_loss: 0.673\n",
            "Epoch[111], train_loss: 0.63483, val_loss: 0.67269\n",
            "Epoch[112], train_loss: 0.63453, val_loss: 0.67239\n",
            "Epoch[113], train_loss: 0.63422, val_loss: 0.67207\n",
            "Epoch[114], train_loss: 0.63392, val_loss: 0.67176\n",
            "Epoch[115], train_loss: 0.63361, val_loss: 0.67144\n",
            "Epoch[116], train_loss: 0.6333, val_loss: 0.67112\n",
            "Epoch[117], train_loss: 0.63299, val_loss: 0.67081\n",
            "Epoch[118], train_loss: 0.63267, val_loss: 0.67049\n",
            "Epoch[119], train_loss: 0.63236, val_loss: 0.67017\n",
            "Epoch[120], train_loss: 0.63204, val_loss: 0.66985\n",
            "Epoch[121], train_loss: 0.63172, val_loss: 0.66953\n",
            "Epoch[122], train_loss: 0.63139, val_loss: 0.6692\n",
            "Epoch[123], train_loss: 0.63107, val_loss: 0.66887\n",
            "Epoch[124], train_loss: 0.63075, val_loss: 0.66855\n",
            "Epoch[125], train_loss: 0.63042, val_loss: 0.66822\n",
            "Epoch[126], train_loss: 0.63009, val_loss: 0.66789\n",
            "Epoch[127], train_loss: 0.62975, val_loss: 0.66756\n",
            "Epoch[128], train_loss: 0.62942, val_loss: 0.66722\n",
            "Epoch[129], train_loss: 0.62909, val_loss: 0.66689\n",
            "Epoch[130], train_loss: 0.62875, val_loss: 0.66655\n",
            "Epoch[131], train_loss: 0.62841, val_loss: 0.66621\n",
            "Epoch[132], train_loss: 0.62807, val_loss: 0.66586\n",
            "Epoch[133], train_loss: 0.62772, val_loss: 0.66552\n",
            "Epoch[134], train_loss: 0.62736, val_loss: 0.66517\n",
            "Epoch[135], train_loss: 0.627, val_loss: 0.66482\n",
            "Epoch[136], train_loss: 0.62664, val_loss: 0.66447\n",
            "Epoch[137], train_loss: 0.62628, val_loss: 0.66411\n",
            "Epoch[138], train_loss: 0.62592, val_loss: 0.66376\n",
            "Epoch[139], train_loss: 0.62556, val_loss: 0.6634\n",
            "Epoch[140], train_loss: 0.6252, val_loss: 0.66304\n",
            "Epoch[141], train_loss: 0.62483, val_loss: 0.66268\n",
            "Epoch[142], train_loss: 0.62447, val_loss: 0.66232\n",
            "Epoch[143], train_loss: 0.6241, val_loss: 0.66195\n",
            "Epoch[144], train_loss: 0.62373, val_loss: 0.66159\n",
            "Epoch[145], train_loss: 0.62336, val_loss: 0.66122\n",
            "Epoch[146], train_loss: 0.62299, val_loss: 0.66085\n",
            "Epoch[147], train_loss: 0.62262, val_loss: 0.66048\n",
            "Epoch[148], train_loss: 0.62225, val_loss: 0.6601\n",
            "Epoch[149], train_loss: 0.62187, val_loss: 0.65973\n",
            "Epoch[150], train_loss: 0.6215, val_loss: 0.65936\n",
            "Epoch[151], train_loss: 0.62112, val_loss: 0.65898\n",
            "Epoch[152], train_loss: 0.62074, val_loss: 0.6586\n",
            "Epoch[153], train_loss: 0.62035, val_loss: 0.65823\n",
            "Epoch[154], train_loss: 0.61997, val_loss: 0.65785\n",
            "Epoch[155], train_loss: 0.61959, val_loss: 0.65747\n",
            "Epoch[156], train_loss: 0.6192, val_loss: 0.65709\n",
            "Epoch[157], train_loss: 0.61882, val_loss: 0.65671\n",
            "Epoch[158], train_loss: 0.61843, val_loss: 0.65633\n",
            "Epoch[159], train_loss: 0.61804, val_loss: 0.65595\n",
            "Epoch[160], train_loss: 0.61765, val_loss: 0.65556\n",
            "Epoch[161], train_loss: 0.61726, val_loss: 0.65518\n",
            "Epoch[162], train_loss: 0.61687, val_loss: 0.6548\n",
            "Epoch[163], train_loss: 0.61648, val_loss: 0.65441\n",
            "Epoch[164], train_loss: 0.61609, val_loss: 0.65402\n",
            "Epoch[165], train_loss: 0.61569, val_loss: 0.65363\n",
            "Epoch[166], train_loss: 0.61529, val_loss: 0.65324\n",
            "Epoch[167], train_loss: 0.61489, val_loss: 0.65285\n",
            "Epoch[168], train_loss: 0.61449, val_loss: 0.65245\n",
            "Epoch[169], train_loss: 0.61409, val_loss: 0.65206\n",
            "Epoch[170], train_loss: 0.61369, val_loss: 0.65166\n",
            "Epoch[171], train_loss: 0.61328, val_loss: 0.65126\n",
            "Epoch[172], train_loss: 0.61288, val_loss: 0.65087\n",
            "Epoch[173], train_loss: 0.61247, val_loss: 0.65047\n",
            "Epoch[174], train_loss: 0.61207, val_loss: 0.65007\n",
            "Epoch[175], train_loss: 0.61166, val_loss: 0.64969\n",
            "Epoch[176], train_loss: 0.61126, val_loss: 0.6493\n",
            "Epoch[177], train_loss: 0.61086, val_loss: 0.64891\n",
            "Epoch[178], train_loss: 0.61046, val_loss: 0.64852\n",
            "Epoch[179], train_loss: 0.61005, val_loss: 0.64813\n",
            "Epoch[180], train_loss: 0.60964, val_loss: 0.64774\n",
            "Epoch[181], train_loss: 0.60924, val_loss: 0.64734\n",
            "Epoch[182], train_loss: 0.60883, val_loss: 0.64695\n",
            "Epoch[183], train_loss: 0.60843, val_loss: 0.64656\n",
            "Epoch[184], train_loss: 0.60802, val_loss: 0.64616\n",
            "Epoch[185], train_loss: 0.60761, val_loss: 0.64576\n",
            "Epoch[186], train_loss: 0.6072, val_loss: 0.64535\n",
            "Epoch[187], train_loss: 0.60678, val_loss: 0.64495\n",
            "Epoch[188], train_loss: 0.60638, val_loss: 0.64455\n",
            "Epoch[189], train_loss: 0.60597, val_loss: 0.64415\n",
            "Epoch[190], train_loss: 0.60556, val_loss: 0.64375\n",
            "Epoch[191], train_loss: 0.60515, val_loss: 0.64336\n",
            "Epoch[192], train_loss: 0.60474, val_loss: 0.64296\n",
            "Epoch[193], train_loss: 0.60433, val_loss: 0.64257\n",
            "Epoch[194], train_loss: 0.60393, val_loss: 0.64217\n",
            "Epoch[195], train_loss: 0.60353, val_loss: 0.64178\n",
            "Epoch[196], train_loss: 0.60313, val_loss: 0.64139\n",
            "Epoch[197], train_loss: 0.60273, val_loss: 0.641\n",
            "Epoch[198], train_loss: 0.60233, val_loss: 0.64061\n",
            "Epoch[199], train_loss: 0.60194, val_loss: 0.64022\n",
            "Epoch[200], train_loss: 0.60154, val_loss: 0.63984\n",
            "Epoch[201], train_loss: 0.60114, val_loss: 0.63946\n",
            "Epoch[202], train_loss: 0.60074, val_loss: 0.63908\n",
            "Epoch[203], train_loss: 0.60035, val_loss: 0.63869\n",
            "Epoch[204], train_loss: 0.59995, val_loss: 0.63831\n",
            "Epoch[205], train_loss: 0.59956, val_loss: 0.63793\n",
            "Epoch[206], train_loss: 0.59917, val_loss: 0.63756\n",
            "Epoch[207], train_loss: 0.59878, val_loss: 0.63718\n",
            "Epoch[208], train_loss: 0.59839, val_loss: 0.6368\n",
            "Epoch[209], train_loss: 0.598, val_loss: 0.63643\n",
            "Epoch[210], train_loss: 0.59762, val_loss: 0.63605\n",
            "Epoch[211], train_loss: 0.59723, val_loss: 0.63567\n",
            "Epoch[212], train_loss: 0.59684, val_loss: 0.63529\n",
            "Epoch[213], train_loss: 0.59645, val_loss: 0.6349\n",
            "Epoch[214], train_loss: 0.59606, val_loss: 0.63452\n",
            "Epoch[215], train_loss: 0.59567, val_loss: 0.63413\n",
            "Epoch[216], train_loss: 0.59528, val_loss: 0.63375\n",
            "Epoch[217], train_loss: 0.59488, val_loss: 0.63336\n",
            "Epoch[218], train_loss: 0.59449, val_loss: 0.63297\n",
            "Epoch[219], train_loss: 0.5941, val_loss: 0.63258\n",
            "Epoch[220], train_loss: 0.59371, val_loss: 0.63219\n",
            "Epoch[221], train_loss: 0.59332, val_loss: 0.6318\n",
            "Epoch[222], train_loss: 0.59293, val_loss: 0.63141\n",
            "Epoch[223], train_loss: 0.59254, val_loss: 0.63102\n",
            "Epoch[224], train_loss: 0.59214, val_loss: 0.63063\n",
            "Epoch[225], train_loss: 0.59175, val_loss: 0.63023\n",
            "Epoch[226], train_loss: 0.59135, val_loss: 0.62983\n",
            "Epoch[227], train_loss: 0.59096, val_loss: 0.62942\n",
            "Epoch[228], train_loss: 0.59056, val_loss: 0.62902\n",
            "Epoch[229], train_loss: 0.59016, val_loss: 0.62862\n",
            "Epoch[230], train_loss: 0.58977, val_loss: 0.62821\n",
            "Epoch[231], train_loss: 0.58937, val_loss: 0.6278\n",
            "Epoch[232], train_loss: 0.58897, val_loss: 0.62739\n",
            "Epoch[233], train_loss: 0.58856, val_loss: 0.62697\n",
            "Epoch[234], train_loss: 0.58814, val_loss: 0.62654\n",
            "Epoch[235], train_loss: 0.58772, val_loss: 0.62611\n",
            "Epoch[236], train_loss: 0.58729, val_loss: 0.62567\n",
            "Epoch[237], train_loss: 0.58686, val_loss: 0.62522\n",
            "Epoch[238], train_loss: 0.58642, val_loss: 0.62476\n",
            "Epoch[239], train_loss: 0.58597, val_loss: 0.62429\n",
            "Epoch[240], train_loss: 0.58552, val_loss: 0.62382\n",
            "Epoch[241], train_loss: 0.58507, val_loss: 0.62334\n",
            "Epoch[242], train_loss: 0.5846, val_loss: 0.62285\n",
            "Epoch[243], train_loss: 0.58413, val_loss: 0.62237\n",
            "Epoch[244], train_loss: 0.58366, val_loss: 0.62187\n",
            "Epoch[245], train_loss: 0.58318, val_loss: 0.62137\n",
            "Epoch[246], train_loss: 0.58269, val_loss: 0.62086\n",
            "Epoch[247], train_loss: 0.5822, val_loss: 0.62035\n",
            "Epoch[248], train_loss: 0.58169, val_loss: 0.61983\n",
            "Epoch[249], train_loss: 0.58119, val_loss: 0.6193\n",
            "Epoch[250], train_loss: 0.58067, val_loss: 0.61876\n",
            "Epoch[251], train_loss: 0.58014, val_loss: 0.61823\n",
            "Epoch[252], train_loss: 0.57961, val_loss: 0.61769\n",
            "Epoch[253], train_loss: 0.57908, val_loss: 0.61714\n",
            "Epoch[254], train_loss: 0.57854, val_loss: 0.6166\n",
            "Epoch[255], train_loss: 0.578, val_loss: 0.61605\n",
            "Epoch[256], train_loss: 0.57746, val_loss: 0.61549\n",
            "Epoch[257], train_loss: 0.57691, val_loss: 0.61494\n",
            "Epoch[258], train_loss: 0.57635, val_loss: 0.61438\n",
            "Epoch[259], train_loss: 0.57579, val_loss: 0.61382\n",
            "Epoch[260], train_loss: 0.57524, val_loss: 0.61325\n",
            "Epoch[261], train_loss: 0.57468, val_loss: 0.6127\n",
            "Epoch[262], train_loss: 0.57413, val_loss: 0.61213\n",
            "Epoch[263], train_loss: 0.57358, val_loss: 0.61157\n",
            "Epoch[264], train_loss: 0.57303, val_loss: 0.61101\n",
            "Epoch[265], train_loss: 0.57247, val_loss: 0.61044\n",
            "Epoch[266], train_loss: 0.57192, val_loss: 0.60987\n",
            "Epoch[267], train_loss: 0.57137, val_loss: 0.60931\n",
            "Epoch[268], train_loss: 0.57083, val_loss: 0.60875\n",
            "Epoch[269], train_loss: 0.57029, val_loss: 0.6082\n",
            "Epoch[270], train_loss: 0.56974, val_loss: 0.60765\n",
            "Epoch[271], train_loss: 0.5692, val_loss: 0.60711\n",
            "Epoch[272], train_loss: 0.56867, val_loss: 0.60657\n",
            "Epoch[273], train_loss: 0.56815, val_loss: 0.60603\n",
            "Epoch[274], train_loss: 0.56763, val_loss: 0.6055\n",
            "Epoch[275], train_loss: 0.56711, val_loss: 0.60497\n",
            "Epoch[276], train_loss: 0.56659, val_loss: 0.60445\n",
            "Epoch[277], train_loss: 0.56607, val_loss: 0.60393\n",
            "Epoch[278], train_loss: 0.56557, val_loss: 0.60343\n",
            "Epoch[279], train_loss: 0.56507, val_loss: 0.60293\n",
            "Epoch[280], train_loss: 0.56458, val_loss: 0.60244\n",
            "Epoch[281], train_loss: 0.56409, val_loss: 0.60196\n",
            "Epoch[282], train_loss: 0.56361, val_loss: 0.60148\n",
            "Epoch[283], train_loss: 0.56314, val_loss: 0.60101\n",
            "Epoch[284], train_loss: 0.56268, val_loss: 0.60055\n",
            "Epoch[285], train_loss: 0.56223, val_loss: 0.60011\n",
            "Epoch[286], train_loss: 0.56179, val_loss: 0.59967\n",
            "Epoch[287], train_loss: 0.56135, val_loss: 0.59924\n",
            "Epoch[288], train_loss: 0.56093, val_loss: 0.59883\n",
            "Epoch[289], train_loss: 0.56052, val_loss: 0.59841\n",
            "Epoch[290], train_loss: 0.56011, val_loss: 0.59799\n",
            "Epoch[291], train_loss: 0.55971, val_loss: 0.59758\n",
            "Epoch[292], train_loss: 0.55931, val_loss: 0.59718\n",
            "Epoch[293], train_loss: 0.55892, val_loss: 0.59678\n",
            "Epoch[294], train_loss: 0.55855, val_loss: 0.5964\n",
            "Epoch[295], train_loss: 0.55819, val_loss: 0.59603\n",
            "Epoch[296], train_loss: 0.55782, val_loss: 0.59566\n",
            "Epoch[297], train_loss: 0.55747, val_loss: 0.59531\n",
            "Epoch[298], train_loss: 0.55712, val_loss: 0.59496\n",
            "Epoch[299], train_loss: 0.55678, val_loss: 0.5946\n",
            "Epoch[300], train_loss: 0.55643, val_loss: 0.59426\n",
            "Epoch[301], train_loss: 0.55609, val_loss: 0.59391\n",
            "Epoch[302], train_loss: 0.55575, val_loss: 0.59358\n",
            "Epoch[303], train_loss: 0.55542, val_loss: 0.59324\n",
            "Epoch[304], train_loss: 0.55508, val_loss: 0.59292\n",
            "Epoch[305], train_loss: 0.55477, val_loss: 0.59261\n",
            "Epoch[306], train_loss: 0.55446, val_loss: 0.5923\n",
            "Epoch[307], train_loss: 0.55415, val_loss: 0.592\n",
            "Epoch[308], train_loss: 0.55384, val_loss: 0.5917\n",
            "Epoch[309], train_loss: 0.55354, val_loss: 0.59139\n",
            "Epoch[310], train_loss: 0.55324, val_loss: 0.5911\n",
            "Epoch[311], train_loss: 0.55294, val_loss: 0.59081\n",
            "Epoch[312], train_loss: 0.55264, val_loss: 0.59053\n",
            "Epoch[313], train_loss: 0.55235, val_loss: 0.59025\n",
            "Epoch[314], train_loss: 0.55207, val_loss: 0.58998\n",
            "Epoch[315], train_loss: 0.55179, val_loss: 0.58971\n",
            "Epoch[316], train_loss: 0.55152, val_loss: 0.58945\n",
            "Epoch[317], train_loss: 0.55125, val_loss: 0.58919\n",
            "Epoch[318], train_loss: 0.55098, val_loss: 0.58893\n",
            "Epoch[319], train_loss: 0.55072, val_loss: 0.58868\n",
            "Epoch[320], train_loss: 0.55047, val_loss: 0.58843\n",
            "Epoch[321], train_loss: 0.55022, val_loss: 0.58819\n",
            "Epoch[322], train_loss: 0.54998, val_loss: 0.58795\n",
            "Epoch[323], train_loss: 0.54974, val_loss: 0.58772\n",
            "Epoch[324], train_loss: 0.5495, val_loss: 0.58749\n",
            "Epoch[325], train_loss: 0.54927, val_loss: 0.58727\n",
            "Epoch[326], train_loss: 0.54904, val_loss: 0.58705\n",
            "Epoch[327], train_loss: 0.54882, val_loss: 0.58684\n",
            "Epoch[328], train_loss: 0.5486, val_loss: 0.58663\n",
            "Epoch[329], train_loss: 0.54839, val_loss: 0.58643\n",
            "Epoch[330], train_loss: 0.54818, val_loss: 0.58622\n",
            "Epoch[331], train_loss: 0.54798, val_loss: 0.58602\n",
            "Epoch[332], train_loss: 0.54778, val_loss: 0.58582\n",
            "Epoch[333], train_loss: 0.54758, val_loss: 0.58562\n",
            "Epoch[334], train_loss: 0.54739, val_loss: 0.58542\n",
            "Epoch[335], train_loss: 0.5472, val_loss: 0.58522\n",
            "Epoch[336], train_loss: 0.54701, val_loss: 0.58504\n",
            "Epoch[337], train_loss: 0.54682, val_loss: 0.58485\n",
            "Epoch[338], train_loss: 0.54664, val_loss: 0.58467\n",
            "Epoch[339], train_loss: 0.54646, val_loss: 0.58448\n",
            "Epoch[340], train_loss: 0.54628, val_loss: 0.5843\n",
            "Epoch[341], train_loss: 0.54609, val_loss: 0.58412\n",
            "Epoch[342], train_loss: 0.54591, val_loss: 0.58394\n",
            "Epoch[343], train_loss: 0.54574, val_loss: 0.58376\n",
            "Epoch[344], train_loss: 0.54556, val_loss: 0.58358\n",
            "Epoch[345], train_loss: 0.54539, val_loss: 0.5834\n",
            "Epoch[346], train_loss: 0.54521, val_loss: 0.58323\n",
            "Epoch[347], train_loss: 0.54504, val_loss: 0.58306\n",
            "Epoch[348], train_loss: 0.54487, val_loss: 0.5829\n",
            "Epoch[349], train_loss: 0.5447, val_loss: 0.58273\n",
            "Epoch[350], train_loss: 0.54453, val_loss: 0.58256\n",
            "Epoch[351], train_loss: 0.54437, val_loss: 0.58239\n",
            "Epoch[352], train_loss: 0.54421, val_loss: 0.58223\n",
            "Epoch[353], train_loss: 0.54404, val_loss: 0.58207\n",
            "Epoch[354], train_loss: 0.54388, val_loss: 0.58191\n",
            "Epoch[355], train_loss: 0.54373, val_loss: 0.58175\n",
            "Epoch[356], train_loss: 0.54357, val_loss: 0.58159\n",
            "Epoch[357], train_loss: 0.54341, val_loss: 0.58144\n",
            "Epoch[358], train_loss: 0.54326, val_loss: 0.58129\n",
            "Epoch[359], train_loss: 0.5431, val_loss: 0.58114\n",
            "Epoch[360], train_loss: 0.54295, val_loss: 0.581\n",
            "Epoch[361], train_loss: 0.5428, val_loss: 0.58085\n",
            "Epoch[362], train_loss: 0.54265, val_loss: 0.58071\n",
            "Epoch[363], train_loss: 0.5425, val_loss: 0.58056\n",
            "Epoch[364], train_loss: 0.54236, val_loss: 0.58042\n",
            "Epoch[365], train_loss: 0.54221, val_loss: 0.58027\n",
            "Epoch[366], train_loss: 0.54206, val_loss: 0.58013\n",
            "Epoch[367], train_loss: 0.54192, val_loss: 0.57999\n",
            "Epoch[368], train_loss: 0.54177, val_loss: 0.57985\n",
            "Epoch[369], train_loss: 0.54163, val_loss: 0.57971\n",
            "Epoch[370], train_loss: 0.54149, val_loss: 0.57957\n",
            "Epoch[371], train_loss: 0.54135, val_loss: 0.57944\n",
            "Epoch[372], train_loss: 0.54122, val_loss: 0.5793\n",
            "Epoch[373], train_loss: 0.54108, val_loss: 0.57917\n",
            "Epoch[374], train_loss: 0.54095, val_loss: 0.57904\n",
            "Epoch[375], train_loss: 0.54082, val_loss: 0.57891\n",
            "Epoch[376], train_loss: 0.54068, val_loss: 0.57879\n",
            "Epoch[377], train_loss: 0.54055, val_loss: 0.57866\n",
            "Epoch[378], train_loss: 0.54042, val_loss: 0.57854\n",
            "Epoch[379], train_loss: 0.5403, val_loss: 0.57842\n",
            "Epoch[380], train_loss: 0.54017, val_loss: 0.5783\n",
            "Epoch[381], train_loss: 0.54005, val_loss: 0.57818\n",
            "Epoch[382], train_loss: 0.53992, val_loss: 0.57807\n",
            "Epoch[383], train_loss: 0.5398, val_loss: 0.57795\n",
            "Epoch[384], train_loss: 0.53968, val_loss: 0.57784\n",
            "Epoch[385], train_loss: 0.53956, val_loss: 0.57772\n",
            "Epoch[386], train_loss: 0.53943, val_loss: 0.57761\n",
            "Epoch[387], train_loss: 0.53931, val_loss: 0.5775\n",
            "Epoch[388], train_loss: 0.53919, val_loss: 0.57739\n",
            "Epoch[389], train_loss: 0.53907, val_loss: 0.57728\n",
            "Epoch[390], train_loss: 0.53896, val_loss: 0.57718\n",
            "Epoch[391], train_loss: 0.53884, val_loss: 0.57707\n",
            "Epoch[392], train_loss: 0.53873, val_loss: 0.57697\n",
            "Epoch[393], train_loss: 0.53862, val_loss: 0.57687\n",
            "Epoch[394], train_loss: 0.5385, val_loss: 0.57676\n",
            "Epoch[395], train_loss: 0.53839, val_loss: 0.57666\n",
            "Epoch[396], train_loss: 0.53828, val_loss: 0.57656\n",
            "Epoch[397], train_loss: 0.53817, val_loss: 0.57646\n",
            "Epoch[398], train_loss: 0.53805, val_loss: 0.57637\n",
            "Epoch[399], train_loss: 0.53794, val_loss: 0.57626\n",
            "Time Taken = 543.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5992c7e0",
      "metadata": {
        "id": "5992c7e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fa251b-26c6-4f74-9799-f5b7cb478a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model Summary ---\n",
            "GNNModel(\n",
            "  (conv1): GraphConv(5, 8)\n",
            "  (conv2): GraphConv(8, 8)\n",
            "  (conv_out): GraphConv(8, 2)\n",
            "  (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu1): ReLU()\n",
            "  (relu2): ReLU()\n",
            "  (linear): Linear(in_features=112, out_features=28, bias=True)\n",
            ")\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "--- Detailed Parameter Summary ---\n",
            "conv1.lin_rel.weight                               [8, 5]               40\n",
            "conv1.lin_rel.bias                                 [8]                  8\n",
            "conv1.lin_root.weight                              [8, 5]               40\n",
            "conv2.lin_rel.weight                               [8, 8]               64\n",
            "conv2.lin_rel.bias                                 [8]                  8\n",
            "conv2.lin_root.weight                              [8, 8]               64\n",
            "conv_out.lin_rel.weight                            [2, 8]               16\n",
            "conv_out.lin_rel.bias                              [2]                  2\n",
            "conv_out.lin_root.weight                           [2, 8]               16\n",
            "bn1.weight                                         [8]                  8\n",
            "bn1.bias                                           [8]                  8\n",
            "bn2.weight                                         [8]                  8\n",
            "bn2.bias                                           [8]                  8\n",
            "linear.weight                                      [28, 112]            3136\n",
            "linear.bias                                        [28]                 28\n",
            "\n",
            " Total Trainable Params: 3454\n",
            "\n",
            "====================================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[0], train_loss: 0.96274, val_loss: 0.96639\n",
            "Epoch[1], train_loss: 0.89874, val_loss: 0.91871\n",
            "Epoch[2], train_loss: 0.86223, val_loss: 0.88536\n",
            "Epoch[3], train_loss: 0.83362, val_loss: 0.85515\n",
            "Epoch[4], train_loss: 0.80602, val_loss: 0.82415\n",
            "Epoch[5], train_loss: 0.7772, val_loss: 0.79121\n",
            "Epoch[6], train_loss: 0.74659, val_loss: 0.75625\n",
            "Epoch[7], train_loss: 0.71438, val_loss: 0.71991\n",
            "Epoch[8], train_loss: 0.68117, val_loss: 0.68288\n",
            "Epoch[9], train_loss: 0.6476, val_loss: 0.6459\n",
            "Epoch[10], train_loss: 0.61432, val_loss: 0.60972\n",
            "Epoch[11], train_loss: 0.58202, val_loss: 0.57502\n",
            "Epoch[12], train_loss: 0.55118, val_loss: 0.54228\n",
            "Epoch[13], train_loss: 0.52216, val_loss: 0.51178\n",
            "Epoch[14], train_loss: 0.49509, val_loss: 0.48365\n",
            "Epoch[15], train_loss: 0.47001, val_loss: 0.45784\n",
            "Epoch[16], train_loss: 0.44683, val_loss: 0.43411\n",
            "Epoch[17], train_loss: 0.42528, val_loss: 0.41222\n",
            "Epoch[18], train_loss: 0.40512, val_loss: 0.39194\n",
            "Epoch[19], train_loss: 0.38619, val_loss: 0.37306\n",
            "Epoch[20], train_loss: 0.36837, val_loss: 0.35543\n",
            "Epoch[21], train_loss: 0.35155, val_loss: 0.3389\n",
            "Epoch[22], train_loss: 0.33568, val_loss: 0.32339\n",
            "Epoch[23], train_loss: 0.32069, val_loss: 0.3088\n",
            "Epoch[24], train_loss: 0.30653, val_loss: 0.29507\n",
            "Epoch[25], train_loss: 0.29319, val_loss: 0.28221\n",
            "Epoch[26], train_loss: 0.28064, val_loss: 0.27014\n",
            "Epoch[27], train_loss: 0.26881, val_loss: 0.25879\n",
            "Epoch[28], train_loss: 0.25763, val_loss: 0.24809\n",
            "Epoch[29], train_loss: 0.24705, val_loss: 0.23799\n",
            "Epoch[30], train_loss: 0.23701, val_loss: 0.22848\n",
            "Epoch[31], train_loss: 0.22752, val_loss: 0.21953\n",
            "Epoch[32], train_loss: 0.21854, val_loss: 0.2111\n",
            "Epoch[33], train_loss: 0.20999, val_loss: 0.20312\n",
            "Epoch[34], train_loss: 0.20183, val_loss: 0.19552\n",
            "Epoch[35], train_loss: 0.19404, val_loss: 0.18825\n",
            "Epoch[36], train_loss: 0.18658, val_loss: 0.18132\n",
            "Epoch[37], train_loss: 0.17942, val_loss: 0.17466\n",
            "Epoch[38], train_loss: 0.17253, val_loss: 0.16826\n",
            "Epoch[39], train_loss: 0.16588, val_loss: 0.16209\n",
            "Epoch[40], train_loss: 0.15949, val_loss: 0.15615\n",
            "Epoch[41], train_loss: 0.15331, val_loss: 0.1504\n",
            "Epoch[42], train_loss: 0.14733, val_loss: 0.14484\n",
            "Epoch[43], train_loss: 0.14151, val_loss: 0.13944\n",
            "Epoch[44], train_loss: 0.13586, val_loss: 0.13419\n",
            "Epoch[45], train_loss: 0.13038, val_loss: 0.12909\n",
            "Epoch[46], train_loss: 0.12507, val_loss: 0.12414\n",
            "Epoch[47], train_loss: 0.11993, val_loss: 0.11933\n",
            "Epoch[48], train_loss: 0.11496, val_loss: 0.11467\n",
            "Epoch[49], train_loss: 0.11016, val_loss: 0.11013\n",
            "Epoch[50], train_loss: 0.10551, val_loss: 0.10574\n",
            "Epoch[51], train_loss: 0.10102, val_loss: 0.10148\n",
            "Epoch[52], train_loss: 0.09669, val_loss: 0.09735\n",
            "Epoch[53], train_loss: 0.0925, val_loss: 0.09336\n",
            "Epoch[54], train_loss: 0.08845, val_loss: 0.0895\n",
            "Epoch[55], train_loss: 0.08456, val_loss: 0.08576\n",
            "Epoch[56], train_loss: 0.08082, val_loss: 0.08216\n",
            "Epoch[57], train_loss: 0.07722, val_loss: 0.07869\n",
            "Epoch[58], train_loss: 0.07374, val_loss: 0.07535\n",
            "Epoch[59], train_loss: 0.0704, val_loss: 0.07214\n",
            "Epoch[60], train_loss: 0.0672, val_loss: 0.06907\n",
            "Epoch[61], train_loss: 0.06417, val_loss: 0.06614\n",
            "Epoch[62], train_loss: 0.06128, val_loss: 0.06334\n",
            "Epoch[63], train_loss: 0.05853, val_loss: 0.06067\n",
            "Epoch[64], train_loss: 0.05592, val_loss: 0.05812\n",
            "Epoch[65], train_loss: 0.05344, val_loss: 0.0557\n",
            "Epoch[66], train_loss: 0.05109, val_loss: 0.05341\n",
            "Epoch[67], train_loss: 0.04886, val_loss: 0.05123\n",
            "Epoch[68], train_loss: 0.04676, val_loss: 0.04918\n",
            "Epoch[69], train_loss: 0.04478, val_loss: 0.04723\n",
            "Epoch[70], train_loss: 0.04293, val_loss: 0.04539\n",
            "Epoch[71], train_loss: 0.04117, val_loss: 0.04366\n",
            "Epoch[72], train_loss: 0.03952, val_loss: 0.04202\n",
            "Epoch[73], train_loss: 0.03797, val_loss: 0.04047\n",
            "Epoch[74], train_loss: 0.03652, val_loss: 0.03901\n",
            "Epoch[75], train_loss: 0.03516, val_loss: 0.03763\n",
            "Epoch[76], train_loss: 0.03388, val_loss: 0.03633\n",
            "Epoch[77], train_loss: 0.03268, val_loss: 0.03511\n",
            "Epoch[78], train_loss: 0.03156, val_loss: 0.03395\n",
            "Epoch[79], train_loss: 0.03049, val_loss: 0.03286\n",
            "Epoch[80], train_loss: 0.02949, val_loss: 0.03182\n",
            "Epoch[81], train_loss: 0.02855, val_loss: 0.03085\n",
            "Epoch[82], train_loss: 0.02766, val_loss: 0.02992\n",
            "Epoch[83], train_loss: 0.02682, val_loss: 0.02905\n",
            "Epoch[84], train_loss: 0.02603, val_loss: 0.02822\n",
            "Epoch[85], train_loss: 0.02528, val_loss: 0.02743\n",
            "Epoch[86], train_loss: 0.02457, val_loss: 0.02668\n",
            "Epoch[87], train_loss: 0.02389, val_loss: 0.02597\n",
            "Epoch[88], train_loss: 0.02325, val_loss: 0.0253\n",
            "Epoch[89], train_loss: 0.02264, val_loss: 0.02466\n",
            "Epoch[90], train_loss: 0.02206, val_loss: 0.02404\n",
            "Epoch[91], train_loss: 0.0215, val_loss: 0.02346\n",
            "Epoch[92], train_loss: 0.02097, val_loss: 0.02289\n",
            "Epoch[93], train_loss: 0.02045, val_loss: 0.02235\n",
            "Epoch[94], train_loss: 0.01996, val_loss: 0.02183\n",
            "Epoch[95], train_loss: 0.01949, val_loss: 0.02134\n",
            "Epoch[96], train_loss: 0.01904, val_loss: 0.02087\n",
            "Epoch[97], train_loss: 0.0186, val_loss: 0.02042\n",
            "Epoch[98], train_loss: 0.01819, val_loss: 0.01999\n",
            "Epoch[99], train_loss: 0.01779, val_loss: 0.01958\n",
            "Epoch[100], train_loss: 0.01742, val_loss: 0.01919\n",
            "Epoch[101], train_loss: 0.01705, val_loss: 0.01881\n",
            "Epoch[102], train_loss: 0.0167, val_loss: 0.01845\n",
            "Epoch[103], train_loss: 0.01637, val_loss: 0.0181\n",
            "Epoch[104], train_loss: 0.01604, val_loss: 0.01777\n",
            "Epoch[105], train_loss: 0.01573, val_loss: 0.01745\n",
            "Epoch[106], train_loss: 0.01543, val_loss: 0.01714\n",
            "Epoch[107], train_loss: 0.01513, val_loss: 0.01684\n",
            "Epoch[108], train_loss: 0.01485, val_loss: 0.01655\n",
            "Epoch[109], train_loss: 0.01458, val_loss: 0.01627\n",
            "Epoch[110], train_loss: 0.01431, val_loss: 0.016\n",
            "Epoch[111], train_loss: 0.01406, val_loss: 0.01574\n",
            "Epoch[112], train_loss: 0.01382, val_loss: 0.01549\n",
            "Epoch[113], train_loss: 0.01358, val_loss: 0.01525\n",
            "Epoch[114], train_loss: 0.01336, val_loss: 0.01502\n",
            "Epoch[115], train_loss: 0.01314, val_loss: 0.01479\n",
            "Epoch[116], train_loss: 0.01292, val_loss: 0.01458\n",
            "Epoch[117], train_loss: 0.01272, val_loss: 0.01436\n",
            "Epoch[118], train_loss: 0.01252, val_loss: 0.01416\n",
            "Epoch[119], train_loss: 0.01232, val_loss: 0.01396\n",
            "Epoch[120], train_loss: 0.01213, val_loss: 0.01377\n",
            "Epoch[121], train_loss: 0.01195, val_loss: 0.01358\n",
            "Epoch[122], train_loss: 0.01177, val_loss: 0.0134\n",
            "Epoch[123], train_loss: 0.0116, val_loss: 0.01322\n",
            "Epoch[124], train_loss: 0.01143, val_loss: 0.01304\n",
            "Epoch[125], train_loss: 0.01126, val_loss: 0.01287\n",
            "Epoch[126], train_loss: 0.0111, val_loss: 0.0127\n",
            "Epoch[127], train_loss: 0.01094, val_loss: 0.01254\n",
            "Epoch[128], train_loss: 0.01078, val_loss: 0.01238\n",
            "Epoch[129], train_loss: 0.01062, val_loss: 0.01222\n",
            "Epoch[130], train_loss: 0.01047, val_loss: 0.01207\n",
            "Epoch[131], train_loss: 0.01033, val_loss: 0.01192\n",
            "Epoch[132], train_loss: 0.01018, val_loss: 0.01178\n",
            "Epoch[133], train_loss: 0.01004, val_loss: 0.01164\n",
            "Epoch[134], train_loss: 0.00991, val_loss: 0.0115\n",
            "Epoch[135], train_loss: 0.00978, val_loss: 0.01137\n",
            "Epoch[136], train_loss: 0.00965, val_loss: 0.01124\n",
            "Epoch[137], train_loss: 0.00953, val_loss: 0.01111\n",
            "Epoch[138], train_loss: 0.00941, val_loss: 0.01099\n",
            "Epoch[139], train_loss: 0.00929, val_loss: 0.01086\n",
            "Epoch[140], train_loss: 0.00917, val_loss: 0.01074\n",
            "Epoch[141], train_loss: 0.00905, val_loss: 0.01063\n",
            "Epoch[142], train_loss: 0.00894, val_loss: 0.01051\n",
            "Epoch[143], train_loss: 0.00883, val_loss: 0.0104\n",
            "Epoch[144], train_loss: 0.00872, val_loss: 0.01029\n",
            "Epoch[145], train_loss: 0.00862, val_loss: 0.01019\n",
            "Epoch[146], train_loss: 0.00851, val_loss: 0.01008\n",
            "Epoch[147], train_loss: 0.00841, val_loss: 0.00997\n",
            "Epoch[148], train_loss: 0.00831, val_loss: 0.00987\n",
            "Epoch[149], train_loss: 0.00821, val_loss: 0.00977\n",
            "Epoch[150], train_loss: 0.00811, val_loss: 0.00966\n",
            "Epoch[151], train_loss: 0.00801, val_loss: 0.00956\n",
            "Epoch[152], train_loss: 0.00791, val_loss: 0.00947\n",
            "Epoch[153], train_loss: 0.00782, val_loss: 0.00937\n",
            "Epoch[154], train_loss: 0.00773, val_loss: 0.00928\n",
            "Epoch[155], train_loss: 0.00764, val_loss: 0.00919\n",
            "Epoch[156], train_loss: 0.00756, val_loss: 0.00909\n",
            "Epoch[157], train_loss: 0.00747, val_loss: 0.009\n",
            "Epoch[158], train_loss: 0.00738, val_loss: 0.00891\n",
            "Epoch[159], train_loss: 0.0073, val_loss: 0.00883\n",
            "Epoch[160], train_loss: 0.00722, val_loss: 0.00874\n",
            "Epoch[161], train_loss: 0.00714, val_loss: 0.00865\n",
            "Epoch[162], train_loss: 0.00706, val_loss: 0.00857\n",
            "Epoch[163], train_loss: 0.00699, val_loss: 0.00848\n",
            "Epoch[164], train_loss: 0.00691, val_loss: 0.0084\n",
            "Epoch[165], train_loss: 0.00683, val_loss: 0.00832\n",
            "Epoch[166], train_loss: 0.00676, val_loss: 0.00823\n",
            "Epoch[167], train_loss: 0.00668, val_loss: 0.00815\n",
            "Epoch[168], train_loss: 0.00661, val_loss: 0.00808\n",
            "Epoch[169], train_loss: 0.00654, val_loss: 0.00799\n",
            "Epoch[170], train_loss: 0.00647, val_loss: 0.00792\n",
            "Epoch[171], train_loss: 0.0064, val_loss: 0.00784\n",
            "Epoch[172], train_loss: 0.00633, val_loss: 0.00776\n",
            "Epoch[173], train_loss: 0.00626, val_loss: 0.00768\n",
            "Epoch[174], train_loss: 0.0062, val_loss: 0.00761\n",
            "Epoch[175], train_loss: 0.00613, val_loss: 0.00753\n",
            "Epoch[176], train_loss: 0.00606, val_loss: 0.00745\n",
            "Epoch[177], train_loss: 0.006, val_loss: 0.00737\n",
            "Epoch[178], train_loss: 0.00593, val_loss: 0.0073\n",
            "Epoch[179], train_loss: 0.00587, val_loss: 0.00723\n",
            "Epoch[180], train_loss: 0.0058, val_loss: 0.00715\n",
            "Epoch[181], train_loss: 0.00574, val_loss: 0.00708\n",
            "Epoch[182], train_loss: 0.00567, val_loss: 0.00701\n",
            "Epoch[183], train_loss: 0.00561, val_loss: 0.00694\n",
            "Epoch[184], train_loss: 0.00555, val_loss: 0.00688\n",
            "Epoch[185], train_loss: 0.00549, val_loss: 0.00681\n",
            "Epoch[186], train_loss: 0.00543, val_loss: 0.00674\n",
            "Epoch[187], train_loss: 0.00537, val_loss: 0.00668\n",
            "Epoch[188], train_loss: 0.00531, val_loss: 0.00661\n",
            "Epoch[189], train_loss: 0.00526, val_loss: 0.00655\n",
            "Epoch[190], train_loss: 0.0052, val_loss: 0.00649\n",
            "Epoch[191], train_loss: 0.00515, val_loss: 0.00642\n",
            "Epoch[192], train_loss: 0.00509, val_loss: 0.00636\n",
            "Epoch[193], train_loss: 0.00504, val_loss: 0.0063\n",
            "Epoch[194], train_loss: 0.00498, val_loss: 0.00624\n",
            "Epoch[195], train_loss: 0.00493, val_loss: 0.00618\n",
            "Epoch[196], train_loss: 0.00488, val_loss: 0.00611\n",
            "Epoch[197], train_loss: 0.00483, val_loss: 0.00605\n",
            "Epoch[198], train_loss: 0.00477, val_loss: 0.00599\n",
            "Epoch[199], train_loss: 0.00472, val_loss: 0.00593\n",
            "Epoch[200], train_loss: 0.00467, val_loss: 0.00588\n",
            "Epoch[201], train_loss: 0.00462, val_loss: 0.00582\n",
            "Epoch[202], train_loss: 0.00457, val_loss: 0.00576\n",
            "Epoch[203], train_loss: 0.00453, val_loss: 0.00571\n",
            "Epoch[204], train_loss: 0.00448, val_loss: 0.00565\n",
            "Epoch[205], train_loss: 0.00443, val_loss: 0.00559\n",
            "Epoch[206], train_loss: 0.00439, val_loss: 0.00554\n",
            "Epoch[207], train_loss: 0.00434, val_loss: 0.00549\n",
            "Epoch[208], train_loss: 0.0043, val_loss: 0.00543\n",
            "Epoch[209], train_loss: 0.00425, val_loss: 0.00538\n",
            "Epoch[210], train_loss: 0.00421, val_loss: 0.00533\n",
            "Epoch[211], train_loss: 0.00417, val_loss: 0.00528\n",
            "Epoch[212], train_loss: 0.00413, val_loss: 0.00523\n",
            "Epoch[213], train_loss: 0.00408, val_loss: 0.00518\n",
            "Epoch[214], train_loss: 0.00404, val_loss: 0.00513\n",
            "Epoch[215], train_loss: 0.004, val_loss: 0.00508\n",
            "Epoch[216], train_loss: 0.00396, val_loss: 0.00503\n",
            "Epoch[217], train_loss: 0.00392, val_loss: 0.00498\n",
            "Epoch[218], train_loss: 0.00388, val_loss: 0.00493\n",
            "Epoch[219], train_loss: 0.00384, val_loss: 0.00488\n",
            "Epoch[220], train_loss: 0.0038, val_loss: 0.00484\n",
            "Epoch[221], train_loss: 0.00377, val_loss: 0.00479\n",
            "Epoch[222], train_loss: 0.00373, val_loss: 0.00474\n",
            "Epoch[223], train_loss: 0.00369, val_loss: 0.0047\n",
            "Epoch[224], train_loss: 0.00365, val_loss: 0.00465\n",
            "Epoch[225], train_loss: 0.00362, val_loss: 0.00461\n",
            "Epoch[226], train_loss: 0.00358, val_loss: 0.00457\n",
            "Epoch[227], train_loss: 0.00354, val_loss: 0.00452\n",
            "Epoch[228], train_loss: 0.00351, val_loss: 0.00448\n",
            "Epoch[229], train_loss: 0.00347, val_loss: 0.00444\n",
            "Epoch[230], train_loss: 0.00344, val_loss: 0.0044\n",
            "Epoch[231], train_loss: 0.0034, val_loss: 0.00435\n",
            "Epoch[232], train_loss: 0.00337, val_loss: 0.00431\n",
            "Epoch[233], train_loss: 0.00333, val_loss: 0.00427\n",
            "Epoch[234], train_loss: 0.0033, val_loss: 0.00423\n",
            "Epoch[235], train_loss: 0.00326, val_loss: 0.00419\n",
            "Epoch[236], train_loss: 0.00323, val_loss: 0.00415\n",
            "Epoch[237], train_loss: 0.0032, val_loss: 0.00411\n",
            "Epoch[238], train_loss: 0.00316, val_loss: 0.00408\n",
            "Epoch[239], train_loss: 0.00313, val_loss: 0.00404\n",
            "Epoch[240], train_loss: 0.0031, val_loss: 0.004\n",
            "Epoch[241], train_loss: 0.00307, val_loss: 0.00396\n",
            "Epoch[242], train_loss: 0.00303, val_loss: 0.00392\n",
            "Epoch[243], train_loss: 0.003, val_loss: 0.00389\n",
            "Epoch[244], train_loss: 0.00297, val_loss: 0.00385\n",
            "Epoch[245], train_loss: 0.00293, val_loss: 0.00381\n",
            "Epoch[246], train_loss: 0.0029, val_loss: 0.00378\n",
            "Epoch[247], train_loss: 0.00287, val_loss: 0.00374\n",
            "Epoch[248], train_loss: 0.00284, val_loss: 0.0037\n",
            "Epoch[249], train_loss: 0.0028, val_loss: 0.00367\n",
            "Epoch[250], train_loss: 0.00277, val_loss: 0.00363\n",
            "Epoch[251], train_loss: 0.00274, val_loss: 0.0036\n",
            "Epoch[252], train_loss: 0.00271, val_loss: 0.00356\n",
            "Epoch[253], train_loss: 0.00267, val_loss: 0.00353\n",
            "Epoch[254], train_loss: 0.00264, val_loss: 0.0035\n",
            "Epoch[255], train_loss: 0.00261, val_loss: 0.00347\n",
            "Epoch[256], train_loss: 0.00258, val_loss: 0.00343\n",
            "Epoch[257], train_loss: 0.00255, val_loss: 0.0034\n",
            "Epoch[258], train_loss: 0.00252, val_loss: 0.00337\n",
            "Epoch[259], train_loss: 0.00249, val_loss: 0.00334\n",
            "Epoch[260], train_loss: 0.00246, val_loss: 0.00331\n",
            "Epoch[261], train_loss: 0.00243, val_loss: 0.00327\n",
            "Epoch[262], train_loss: 0.0024, val_loss: 0.00324\n",
            "Epoch[263], train_loss: 0.00237, val_loss: 0.00321\n",
            "Epoch[264], train_loss: 0.00235, val_loss: 0.00318\n",
            "Epoch[265], train_loss: 0.00232, val_loss: 0.00315\n",
            "Epoch[266], train_loss: 0.00229, val_loss: 0.00312\n",
            "Epoch[267], train_loss: 0.00226, val_loss: 0.00309\n",
            "Epoch[268], train_loss: 0.00223, val_loss: 0.00306\n",
            "Epoch[269], train_loss: 0.00221, val_loss: 0.00303\n",
            "Epoch[270], train_loss: 0.00218, val_loss: 0.003\n",
            "Epoch[271], train_loss: 0.00215, val_loss: 0.00297\n",
            "Epoch[272], train_loss: 0.00213, val_loss: 0.00295\n",
            "Epoch[273], train_loss: 0.0021, val_loss: 0.00292\n",
            "Epoch[274], train_loss: 0.00208, val_loss: 0.00289\n",
            "Epoch[275], train_loss: 0.00205, val_loss: 0.00286\n",
            "Epoch[276], train_loss: 0.00203, val_loss: 0.00284\n",
            "Epoch[277], train_loss: 0.002, val_loss: 0.00281\n",
            "Epoch[278], train_loss: 0.00198, val_loss: 0.00278\n",
            "Epoch[279], train_loss: 0.00196, val_loss: 0.00276\n",
            "Epoch[280], train_loss: 0.00193, val_loss: 0.00273\n",
            "Epoch[281], train_loss: 0.00191, val_loss: 0.0027\n",
            "Epoch[282], train_loss: 0.00188, val_loss: 0.00268\n",
            "Epoch[283], train_loss: 0.00186, val_loss: 0.00265\n",
            "Epoch[284], train_loss: 0.00184, val_loss: 0.00262\n",
            "Epoch[285], train_loss: 0.00181, val_loss: 0.0026\n",
            "Epoch[286], train_loss: 0.00179, val_loss: 0.00257\n",
            "Epoch[287], train_loss: 0.00176, val_loss: 0.00254\n",
            "Epoch[288], train_loss: 0.00174, val_loss: 0.00252\n",
            "Epoch[289], train_loss: 0.00172, val_loss: 0.00249\n",
            "Epoch[290], train_loss: 0.00169, val_loss: 0.00247\n",
            "Epoch[291], train_loss: 0.00167, val_loss: 0.00245\n",
            "Epoch[292], train_loss: 0.00165, val_loss: 0.00242\n",
            "Epoch[293], train_loss: 0.00163, val_loss: 0.0024\n",
            "Epoch[294], train_loss: 0.0016, val_loss: 0.00237\n",
            "Epoch[295], train_loss: 0.00158, val_loss: 0.00235\n",
            "Epoch[296], train_loss: 0.00156, val_loss: 0.00233\n",
            "Epoch[297], train_loss: 0.00154, val_loss: 0.00231\n",
            "Epoch[298], train_loss: 0.00152, val_loss: 0.00228\n",
            "Epoch[299], train_loss: 0.0015, val_loss: 0.00226\n",
            "Epoch[300], train_loss: 0.00148, val_loss: 0.00224\n",
            "Epoch[301], train_loss: 0.00146, val_loss: 0.00222\n",
            "Epoch[302], train_loss: 0.00144, val_loss: 0.0022\n",
            "Epoch[303], train_loss: 0.00142, val_loss: 0.00218\n",
            "Epoch[304], train_loss: 0.0014, val_loss: 0.00215\n",
            "Epoch[305], train_loss: 0.00138, val_loss: 0.00213\n",
            "Epoch[306], train_loss: 0.00136, val_loss: 0.00211\n",
            "Epoch[307], train_loss: 0.00134, val_loss: 0.00209\n",
            "Epoch[308], train_loss: 0.00132, val_loss: 0.00207\n",
            "Epoch[309], train_loss: 0.0013, val_loss: 0.00205\n",
            "Epoch[310], train_loss: 0.00128, val_loss: 0.00203\n",
            "Epoch[311], train_loss: 0.00126, val_loss: 0.00201\n",
            "Epoch[312], train_loss: 0.00124, val_loss: 0.00199\n",
            "Epoch[313], train_loss: 0.00122, val_loss: 0.00197\n",
            "Epoch[314], train_loss: 0.0012, val_loss: 0.00195\n",
            "Epoch[315], train_loss: 0.00119, val_loss: 0.00194\n",
            "Epoch[316], train_loss: 0.00117, val_loss: 0.00192\n",
            "Epoch[317], train_loss: 0.00115, val_loss: 0.0019\n",
            "Epoch[318], train_loss: 0.00113, val_loss: 0.00188\n",
            "Epoch[319], train_loss: 0.00111, val_loss: 0.00186\n",
            "Epoch[320], train_loss: 0.0011, val_loss: 0.00185\n",
            "Epoch[321], train_loss: 0.00108, val_loss: 0.00183\n",
            "Epoch[322], train_loss: 0.00106, val_loss: 0.00181\n",
            "Epoch[323], train_loss: 0.00105, val_loss: 0.0018\n",
            "Epoch[324], train_loss: 0.00103, val_loss: 0.00178\n",
            "Epoch[325], train_loss: 0.00101, val_loss: 0.00176\n",
            "Epoch[326], train_loss: 0.001, val_loss: 0.00175\n",
            "Epoch[327], train_loss: 0.00098, val_loss: 0.00173\n",
            "Epoch[328], train_loss: 0.00096, val_loss: 0.00172\n",
            "Epoch[329], train_loss: 0.00095, val_loss: 0.0017\n",
            "Epoch[330], train_loss: 0.00093, val_loss: 0.00169\n",
            "Epoch[331], train_loss: 0.00092, val_loss: 0.00167\n",
            "Epoch[332], train_loss: 0.0009, val_loss: 0.00166\n",
            "Epoch[333], train_loss: 0.00089, val_loss: 0.00164\n",
            "Epoch[334], train_loss: 0.00087, val_loss: 0.00163\n",
            "Epoch[335], train_loss: 0.00086, val_loss: 0.00162\n",
            "Epoch[336], train_loss: 0.00084, val_loss: 0.0016\n",
            "Epoch[337], train_loss: 0.00083, val_loss: 0.00159\n",
            "Epoch[338], train_loss: 0.00081, val_loss: 0.00158\n",
            "Epoch[339], train_loss: 0.0008, val_loss: 0.00156\n",
            "Epoch[340], train_loss: 0.00079, val_loss: 0.00155\n",
            "Epoch[341], train_loss: 0.00077, val_loss: 0.00154\n",
            "Epoch[342], train_loss: 0.00076, val_loss: 0.00152\n",
            "Epoch[343], train_loss: 0.00075, val_loss: 0.00151\n",
            "Epoch[344], train_loss: 0.00073, val_loss: 0.0015\n",
            "Epoch[345], train_loss: 0.00072, val_loss: 0.00149\n",
            "Epoch[346], train_loss: 0.00071, val_loss: 0.00148\n",
            "Epoch[347], train_loss: 0.00069, val_loss: 0.00147\n",
            "Epoch[348], train_loss: 0.00068, val_loss: 0.00145\n",
            "Epoch[349], train_loss: 0.00067, val_loss: 0.00144\n",
            "Epoch[350], train_loss: 0.00066, val_loss: 0.00143\n",
            "Epoch[351], train_loss: 0.00065, val_loss: 0.00142\n",
            "Epoch[352], train_loss: 0.00063, val_loss: 0.00141\n",
            "Epoch[353], train_loss: 0.00062, val_loss: 0.0014\n",
            "Epoch[354], train_loss: 0.00061, val_loss: 0.00139\n",
            "Epoch[355], train_loss: 0.0006, val_loss: 0.00138\n",
            "Epoch[356], train_loss: 0.00059, val_loss: 0.00137\n",
            "Epoch[357], train_loss: 0.00058, val_loss: 0.00136\n",
            "Epoch[358], train_loss: 0.00057, val_loss: 0.00135\n",
            "Epoch[359], train_loss: 0.00056, val_loss: 0.00134\n",
            "Epoch[360], train_loss: 0.00055, val_loss: 0.00133\n",
            "Epoch[361], train_loss: 0.00053, val_loss: 0.00132\n",
            "Epoch[362], train_loss: 0.00052, val_loss: 0.00131\n",
            "Epoch[363], train_loss: 0.00051, val_loss: 0.0013\n",
            "Epoch[364], train_loss: 0.0005, val_loss: 0.00129\n",
            "Epoch[365], train_loss: 0.0005, val_loss: 0.00128\n",
            "Epoch[366], train_loss: 0.00049, val_loss: 0.00127\n",
            "Epoch[367], train_loss: 0.00048, val_loss: 0.00126\n",
            "Epoch[368], train_loss: 0.00047, val_loss: 0.00126\n",
            "Epoch[369], train_loss: 0.00046, val_loss: 0.00125\n",
            "Epoch[370], train_loss: 0.00045, val_loss: 0.00124\n",
            "Epoch[371], train_loss: 0.00044, val_loss: 0.00123\n",
            "Epoch[372], train_loss: 0.00043, val_loss: 0.00122\n",
            "Epoch[373], train_loss: 0.00043, val_loss: 0.00122\n",
            "Epoch[374], train_loss: 0.00042, val_loss: 0.00121\n",
            "Epoch[375], train_loss: 0.00041, val_loss: 0.0012\n",
            "Epoch[376], train_loss: 0.0004, val_loss: 0.00119\n",
            "Epoch[377], train_loss: 0.00039, val_loss: 0.00119\n",
            "Epoch[378], train_loss: 0.00039, val_loss: 0.00118\n",
            "Epoch[379], train_loss: 0.00038, val_loss: 0.00117\n",
            "Epoch[380], train_loss: 0.00037, val_loss: 0.00117\n",
            "Epoch[381], train_loss: 0.00036, val_loss: 0.00116\n",
            "Epoch[382], train_loss: 0.00036, val_loss: 0.00115\n",
            "Epoch[383], train_loss: 0.00035, val_loss: 0.00115\n",
            "Epoch[384], train_loss: 0.00034, val_loss: 0.00114\n",
            "Epoch[385], train_loss: 0.00034, val_loss: 0.00113\n",
            "Epoch[386], train_loss: 0.00033, val_loss: 0.00113\n",
            "Epoch[387], train_loss: 0.00033, val_loss: 0.00112\n",
            "Epoch[388], train_loss: 0.00032, val_loss: 0.00112\n",
            "Epoch[389], train_loss: 0.00031, val_loss: 0.00111\n",
            "Epoch[390], train_loss: 0.00031, val_loss: 0.0011\n",
            "Epoch[391], train_loss: 0.0003, val_loss: 0.0011\n",
            "Epoch[392], train_loss: 0.0003, val_loss: 0.00109\n",
            "Epoch[393], train_loss: 0.00029, val_loss: 0.00109\n",
            "Epoch[394], train_loss: 0.00029, val_loss: 0.00108\n",
            "Epoch[395], train_loss: 0.00028, val_loss: 0.00108\n",
            "Epoch[396], train_loss: 0.00027, val_loss: 0.00107\n",
            "Epoch[397], train_loss: 0.00027, val_loss: 0.00107\n",
            "Epoch[398], train_loss: 0.00026, val_loss: 0.00106\n",
            "Epoch[399], train_loss: 0.00026, val_loss: 0.00106\n",
            "Time Taken = 350.99\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "history_GraphConv = model_run(gnn_type='GraphConv', n_epochs = 400)\n",
        "end = time.time()\n",
        "print(f\"Time Taken = {round(end-start, 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "iterations = np.arange(0, 400)\n",
        "train_loss_gc = [item['train_loss'] for item in history_GraphConv]\n",
        "val_loss_gc = [item['val_loss'] for item in history_GraphConv]\n",
        "plt.plot(iterations, val_loss_gc, label='GraphConv Val Loss')\n",
        "plt.plot(iterations, train_loss_gc, '--', label='GraphConv Train Loss')\n",
        "\n",
        "train_loss_gt = [item['train_loss'] for item in history_transformer]\n",
        "val_loss_gt = [item['val_loss'] for item in history_transformer]\n",
        "plt.plot(iterations, train_loss_gt, label='GraphTransformer Train Loss')\n",
        "plt.plot(iterations, val_loss_gt , label='GraphTransformer Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "S5AIoX_6PYkO",
        "outputId": "e4f6e7fe-3c48-4a90-8c07-01c9f74c32f9"
      },
      "id": "S5AIoX_6PYkO",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b9wTNpTLTjDh"
      },
      "id": "b9wTNpTLTjDh",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PdybNnVqUBOC"
      },
      "id": "PdybNnVqUBOC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}